{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a75301",
   "metadata": {},
   "source": [
    "# ──────────────────────────────\n",
    "# Day 4 — Chain Rule & Gradient Descent\n",
    "# ──────────────────────────────\n",
    "\n",
    "\"\"\"\n",
    "# Day 4 — Chain Rule & Gradient Descent\n",
    "\n",
    "**Author:** Dhairya Patel  \n",
    "\n",
    "This notebook covers:\n",
    "1. Chain Rule examples\n",
    "2. Gradients on nested functions\n",
    "3. Gradient Descent on Linear Regression cost function\n",
    "4. Visualization of cost minimization\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51981e81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6de143",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 1) Chain Rule Examples\n",
    "If y = f(g(x)), then dy/dx = f'(g(x)) * g'(x).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbd9a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x = sp.Symbol('x')\n",
    "inner = 3*x**2 + 2*x\n",
    "outer = (inner)**5\n",
    "derivative = sp.diff(outer, x)\n",
    "outer, derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaac46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 2) Gradient of a Nested Function\n",
    "Let's compute gradient for z = (x^2 + y^2)^3\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5af6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x, y = sp.symbols('x y')\n",
    "f = (x**2 + y**2)**3\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "[df_dx, df_dy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892baa2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 3) Gradient Descent on Linear Regression Cost\n",
    "\n",
    "We minimize MSE: J(m,b) = (1/n) * Σ (y - (mx+b))^2\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a3e13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X = np.array([1,2,3,4,5])\n",
    "Y = np.array([2,4,6,8,10])  # perfect line y=2x\n",
    "\n",
    "# Initialize params\n",
    "m, b = 0.0, 0.0\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "n = len(X)\n",
    "\n",
    "costs = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = m*X + b\n",
    "    error = y_pred - Y\n",
    "    cost = (1/n) * np.sum(error**2)\n",
    "    costs.append(cost)\n",
    "    \n",
    "    # Gradients\n",
    "    dm = (2/n) * np.sum(error * X)\n",
    "    db = (2/n) * np.sum(error)\n",
    "    \n",
    "    # Update\n",
    "    m -= lr * dm\n",
    "    b -= lr * db\n",
    "\n",
    "m, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b57b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.title(\"Cost Function Decreasing with Gradient Descent\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE Cost\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6079756",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- The Chain Rule is the backbone of backpropagation in neural networks.\n",
    "- Gradient Descent allows us to iteratively minimize a cost function.\n",
    "- Learning rate tuning is key to convergence.\n",
    "\n",
    "**End of Day 4.**\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
